{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cdd7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for VAE module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "#imports for training\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#imports for dataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#imports for loss function\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c275f5d",
   "metadata": {},
   "source": [
    "## VAE Module \n",
    "- Input img (x)\n",
    "- Hidden dim\n",
    "- mean, std\n",
    "- paremterization trick\n",
    "- Decoder \n",
    "- Ouput img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module): \n",
    "    \n",
    "    #initilzation functino\n",
    "    def __init__(self, input_dim=784, hidden_dim=200, latent_z_dim=20): \n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        #encoder\n",
    "        self.img_to_hidden = nn.Linear(input_dim, hidden_dim)       #hidden layer\n",
    "        self.hidden_to_mean = nn.Linear(hidden_dim, latent_z_dim)   #mean of the latent space\n",
    "        self.hidden_to_sigma = nn.Linear(hidden_dim, latent_z_dim)  #sigma of the latent space\n",
    "        \n",
    "        #decoder \n",
    "        self.latent_to_hidden = nn.Linear(latent_z_dim, hidden_dim) #hidden layer\n",
    "        self.hidden_to_img = nn.Linear(hidden_dim, input_dim)       #output layer\n",
    "\n",
    "\n",
    "    #encoder which equates to q_phi(z|x)\n",
    "    def encode(self, x): \n",
    "        x = nn.ReLU(self.img_to_hidden(x))\n",
    "        mu, sigma = self.hidden_to_mean(x), self.hidden_to_sigma(x)\n",
    "        return mu, sigma\n",
    "    \n",
    "    #decoder which equates to p_theta(x|z)\n",
    "    def decode(self, z): \n",
    "        z = self.relu(self.latent_to_hidden(z))\n",
    "        x_hat = self.hidden_to_img(z)\n",
    "        \n",
    "        #sigmoid to ensure the output is between 0 and 1\n",
    "        return torch.sigmoid(x_hat)\n",
    "    \n",
    "    #reparameterization trick - helper function\n",
    "    def reparameterize(self, mu, sigma): \n",
    "        epsilon = torch.randn_like(mu)\n",
    "        return mu + sigma * epsilon\n",
    "    \n",
    "    def forward(self, x): \n",
    "        \n",
    "        mu, sigma = self.encode(x)\n",
    "        \n",
    "        #reparameterize the latent space\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        \n",
    "        #decode the latent space where x hat is the reconstructed image\n",
    "        x_hat = self.decode(z)\n",
    "        \n",
    "        #return the reconstructed image, the mean and the sigma\n",
    "        return x_hat, mu, sigma\n",
    "    \n",
    "    #loss function\n",
    "    def loss_function(self, x, x_hat, mu, sigma): \n",
    "        \n",
    "        #reconstruction loss for MNIST Bernoulli distribution\n",
    "        recon_loss = F.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "        \n",
    "        #KL divergence loss\n",
    "        kl_loss = -0.5 * torch.sum(1 + torch.log(sigma**2) - mu**2 - sigma**2)\n",
    "        \n",
    "        #total loss\n",
    "        return recon_loss, kl_loss, recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0736bac0",
   "metadata": {},
   "source": [
    "## Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \n",
    "    def __init__(self, model, device, epochs=10, batch_size=100, learning_rate=0.001, input_dim=784, dataloader=None):\n",
    "        \n",
    "        #hyperparameters\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_dim = input_dim\n",
    "        self.dataloader = dataloader\n",
    "        \n",
    "        #optimizer\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "    def train(self, fixed_x): \n",
    "        \n",
    "        #training loop\n",
    "        for epoch in range(self.epochs): \n",
    "            for i, (x, _) in enumerate(self.dataloader): \n",
    "                \n",
    "                #move to device\n",
    "                x = x.reshape(-1, self.input_dim).to(self.device)\n",
    "                \n",
    "                #reconstruction \n",
    "                x_hat, mu, sigma = self.model(x)\n",
    "                \n",
    "                #loss function\n",
    "                recon_loss, kl_loss, loss = self.model.loss_function(x, x_hat, mu, sigma)\n",
    "                \n",
    "                #backpropagation\n",
    "                self.optimizer.zero_grad()  #zero the gradients\n",
    "                loss.backward()  #backpropagation\n",
    "                self.optimizer.step()  #update the parameters \n",
    "                \n",
    "                #print loss\n",
    "                if i % 20 == 0: \n",
    "                        print(f\"Epoch [{epoch+1}/{self.epochs}] | Batch [{i+1}/{len(self.dataloader)}] | Loss {loss.item():.4f} | Reconstruction Loss {recon_loss.item():.4f} | KL Divergence Loss {kl_loss.item():.4f}\")\n",
    "            \n",
    "            #visulize after each epoch\n",
    "            with torch.no_grad():\n",
    "                recon, _, _, _ = model(fixed_x)\n",
    "                recon = recon.view(-1, 1, 28, 28).cpu()\n",
    "                fig, axes = plt.subplots(2, 16, figsize=(16, 2))\n",
    "                for j in range(16):\n",
    "                    axes[0, j].imshow(fixed_x[j].cpu().view(28, 28), cmap='gray')\n",
    "                    axes[0, j].axis('off')\n",
    "                    axes[1, j].imshow(recon[j].view(28, 28), cmap='gray')\n",
    "                    axes[1, j].axis('off')\n",
    "                plt.suptitle(f\"Top: Original, Bottom: Reconstruction (Epoch {epoch+1})\")\n",
    "                plt.show()\n",
    "                \n",
    "        #return the trained model\n",
    "        return self.model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    #hyperparameters\n",
    "    epochs = 10\n",
    "    batch_size = 100\n",
    "    learning_rate = 0.001\n",
    "    hidden_dim = 400\n",
    "    latent_z_dim = 16\n",
    "    input_dim = 784\n",
    "    \n",
    "    #importing MNIST dataset\n",
    "    dataset = MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "    dataloader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
    "        \n",
    "    #device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #model & trainer module initialization\n",
    "    model = VAE(hidden_dim, latent_z_dim, input_dim).to(device)\n",
    "    \n",
    "    #fixed batch of images\n",
    "    fixed_x, fixed_y = next(iter(dataloader))\n",
    "    fixed_x = fixed_x[:16].reshape(-1, input_dim).to(device)\n",
    "    \n",
    "    trainer = Trainer(model, device, epochs, batch_size, learning_rate, input_dim, dataloader)\n",
    "    \n",
    "    #train the model\n",
    "    trainer.train(fixed_x)\n",
    "    \n",
    "    #save the model to onnx format \n",
    "    torch.onnx.export(model, fixed_x, \"vae.onnx\", verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
