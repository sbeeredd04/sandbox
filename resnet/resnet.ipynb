{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ebe6890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a41d018",
   "metadata": {},
   "source": [
    "## Conv Block Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7bd77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for ResNet 18 and 34\"\"\"\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Shortcut connection to match dimensions\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Add the identity shortcut (x) to the output of conv layers (F(x))\n",
    "        out += self.shortcut(identity)  # This is where H(x) = F(x) + x happens\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989dd02e",
   "metadata": {},
   "source": [
    "## Residual Block Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb0dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"Bottleneck Block for ResNet 50, 101, 152\"\"\"\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Shortcut connection to match dimensions\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        out += self.shortcut(identity)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afafac",
   "metadata": {},
   "source": [
    "## ResNet module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7189c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000, in_channels=3):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        \n",
    "        # Classification layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        \n",
    "        # First block may have a different stride\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        \n",
    "        # Update in_channels for subsequent blocks\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        \n",
    "        # Add the rest of the blocks\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e252ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create different ResNet variants\n",
    "def resnet18(num_classes=1000):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "def resnet34(num_classes=1000):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "def resnet50(num_classes=1000):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "def resnet101(num_classes=1000):\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes)\n",
    "\n",
    "def resnet152(num_classes=1000):\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3], num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cac11e3",
   "metadata": {},
   "source": [
    "# Training a ResNet 34 on CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3602b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ecdc44",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Complete Training Module for ResNet-34 on CIFAR-10\n",
    "\n",
    "This section provides a comprehensive training pipeline with proper data preprocessing, training loop, validation, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "969b7dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and augmentation\n",
    "def get_cifar10_loaders(batch_size=128):\n",
    "    \"\"\"\n",
    "    Get CIFAR-10 train and test data loaders with appropriate transforms\n",
    "    \"\"\"\n",
    "    # Training transforms with data augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),  # Random crop with padding\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # CIFAR-10 normalization\n",
    "    ])\n",
    "    \n",
    "    # Test transforms (no augmentation)\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    # Download and load datasets\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=train_transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=test_transform\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d399deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        # Print progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Batch {batch_idx}/{len(train_loader)}, '\n",
    "                  f'Loss: {loss.item():.4f}, '\n",
    "                  f'Acc: {100.*correct/total:.2f}%')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89aad232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            \n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0edb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "def adjust_learning_rate(optimizer, epoch, initial_lr):\n",
    "    \"\"\"\n",
    "    Adjust learning rate based on epoch (step decay)\n",
    "    \"\"\"\n",
    "    if epoch >= 150:\n",
    "        lr = initial_lr * 0.01  # Reduce by 100x after epoch 150\n",
    "    elif epoch >= 100:\n",
    "        lr = initial_lr * 0.1   # Reduce by 10x after epoch 100\n",
    "    else:\n",
    "        lr = initial_lr\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87798f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete training function\n",
    "def train_resnet34_cifar10(epochs=200, batch_size=128, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Complete training pipeline for ResNet-34 on CIFAR-10\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    # Get data loaders\n",
    "    train_loader, test_loader = get_cifar10_loaders(batch_size)\n",
    "    print(f'Training samples: {len(train_loader.dataset)}')\n",
    "    print(f'Test samples: {len(test_loader.dataset)}')\n",
    "    \n",
    "    # Initialize model\n",
    "    model = resnet34(num_classes=10)  # CIFAR-10 has 10 classes\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'Total parameters: {total_params:,}')\n",
    "    print(f'Trainable parameters: {trainable_params:,}')\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    learning_rates = []\n",
    "    \n",
    "    print('Starting training...')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Adjust learning rate\n",
    "        current_lr = adjust_learning_rate(optimizer, epoch, learning_rate)\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        print(f'\\\\nEpoch {epoch+1}/{epochs}, Learning Rate: {current_lr:.6f}')\n",
    "        \n",
    "        # Train for one epoch\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Store history\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "        \n",
    "        # Save checkpoint every 50 epochs\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'test_loss': test_loss,\n",
    "                'test_acc': test_acc,\n",
    "            }, f'resnet34_cifar10_epoch_{epoch+1}.pth')\n",
    "            print(f'Checkpoint saved at epoch {epoch+1}')\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f'\\\\nTraining completed in {total_time/3600:.2f} hours')\n",
    "    print(f'Best test accuracy: {max(test_accuracies):.2f}%')\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), 'resnet34_cifar10_final.pth')\n",
    "    \n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'test_accuracies': test_accuracies,\n",
    "        'learning_rates': learning_rates\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0817c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function for training curves\n",
    "def plot_training_curves(history):\n",
    "    \"\"\"\n",
    "    Plot training curves\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    train_losses = history['train_losses']\n",
    "    test_losses = history['test_losses']\n",
    "    train_accuracies = history['train_accuracies']\n",
    "    test_accuracies = history['test_accuracies']\n",
    "    learning_rates = history['learning_rates']\n",
    "    \n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "    ax1.plot(epochs, test_losses, 'r-', label='Test Loss')\n",
    "    ax1.set_title('Loss Curves')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
    "    ax2.plot(epochs, test_accuracies, 'r-', label='Test Accuracy')\n",
    "    ax2.set_title('Accuracy Curves')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    ax3.plot(epochs, learning_rates, 'g-')\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    # Test accuracy zoomed\n",
    "    ax4.plot(epochs, test_accuracies, 'r-', linewidth=2)\n",
    "    ax4.set_title('Test Accuracy (Detailed)')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Test Accuracy (%)')\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('resnet34_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5c3a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-wise accuracy analysis\n",
    "def analyze_class_performance(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Analyze per-class performance\n",
    "    \"\"\"\n",
    "    # CIFAR-10 class names\n",
    "    classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    model.eval()\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == target).squeeze()\n",
    "            \n",
    "            for i in range(target.size(0)):\n",
    "                label = target[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    print(\"\\\\nPer-class accuracy:\")\n",
    "    for i in range(10):\n",
    "        accuracy = 100 * class_correct[i] / class_total[i]\n",
    "        print(f'{classes[i]}: {accuracy:.2f}% ({int(class_correct[i])}/{int(class_total[i])})')\n",
    "    \n",
    "    return class_correct, class_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd84d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ResNet-34 training on CIFAR-10...\n",
      "Note: This will take several hours to complete. For quick testing, reduce epochs to 5-10.\n",
      "Using device: cpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training samples: 50000\n",
      "Test samples: 10000\n",
      "Total parameters: 21,289,802\n",
      "Trainable parameters: 21,289,802\n",
      "Starting training...\n",
      "\\nEpoch 1/5, Learning Rate: 0.100000\n",
      "Batch 0/391, Loss: 2.7638, Acc: 7.03%\n",
      "Batch 100/391, Loss: 2.7556, Acc: 12.47%\n",
      "Batch 200/391, Loss: 2.2832, Acc: 14.74%\n",
      "Batch 300/391, Loss: 2.0590, Acc: 16.65%\n",
      "Train Loss: 2.5435, Train Acc: 18.32%\n",
      "Test Loss: 2.1633, Test Acc: 23.55%\n",
      "\\nEpoch 2/5, Learning Rate: 0.100000\n",
      "Batch 0/391, Loss: 2.0806, Acc: 25.00%\n",
      "Batch 100/391, Loss: 1.8509, Acc: 25.87%\n",
      "Batch 200/391, Loss: 1.8104, Acc: 27.44%\n",
      "Batch 300/391, Loss: 1.7905, Acc: 28.76%\n",
      "Train Loss: 1.8569, Train Acc: 29.68%\n",
      "Test Loss: 1.6593, Test Acc: 36.83%\n",
      "\\nEpoch 3/5, Learning Rate: 0.100000\n"
     ]
    }
   ],
   "source": [
    "# Main training execution\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Train the model (uncomment to run full training)\n",
    "print(\"Starting ResNet-34 training on CIFAR-10...\")\n",
    "print(\"Note: This will take several hours to complete. For quick testing, reduce epochs to 5-10.\")\n",
    "\n",
    "# For quick testing (5 epochs)\n",
    "model, history = train_resnet34_cifar10(epochs=5, batch_size=128, learning_rate=0.1)\n",
    "\n",
    "# For full training (uncomment the line below and comment the line above)\n",
    "# model, history = train_resnet34_cifar10(epochs=200, batch_size=128, learning_rate=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb61716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plot_training_curves(history)\n",
    "\n",
    "# Analyze final model performance\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "_, test_loader = get_cifar10_loaders(batch_size=128)\n",
    "\n",
    "print(\"\\\\nFinal model performance:\")\n",
    "analyze_class_performance(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30376666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load and test a saved model\n",
    "def load_and_test_model(model_path):\n",
    "    \"\"\"\n",
    "    Load and test a saved model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load model\n",
    "    model = resnet34(num_classes=10)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Get test loader\n",
    "    _, test_loader = get_cifar10_loaders(batch_size=128)\n",
    "    \n",
    "    # Test the model\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f'Loaded model test accuracy: {test_acc:.2f}%')\n",
    "    \n",
    "    # Analyze class-wise performance\n",
    "    analyze_class_performance(model, test_loader, device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# model = load_and_test_model('resnet34_cifar10_final.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
